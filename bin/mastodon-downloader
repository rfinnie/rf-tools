#!/usr/bin/env python3

# SPDX-FileComment: Mastodon downloader
# SPDX-FileCopyrightText: Copyright (C) 2023 Ryan Finnie
# SPDX-License-Identifier: MPL-2.0

# Sample config.yaml:
#
# site: https://example.com
# api_key: APIKEY
# username: foo
# data_dir: /path/to/data
#
# One or both of username or api_key are required. If username is not
# specified, the API key's logged in user is used.

import argparse
import datetime
import json
import logging
import os
import pathlib
import random
import re
import sys
import time

import dateutil.parser
import requests
import yaml


class MastodonDownloader:
    site = None
    api_key = None
    data_dir = None
    username = None

    re_links = re.compile(r"<(?P<url>.*?)>; rel=\"(?P<link>.*?)\"")
    user_id = None
    args = None

    def __init__(self):
        self.requests_session = requests.Session()

    def get_sleep_time(self, r):
        if not r.headers.get("x-ratelimit-limit"):
            return
        api_current_time = dateutil.parser.parse(r.headers.get("date"))
        api_limit = float(r.headers.get("x-ratelimit-limit"))
        api_remaining = float(r.headers.get("x-ratelimit-remaining"))
        try:
            # Epoch time (GitHub, etc)
            api_reset = datetime.datetime.fromtimestamp(
                float(r.headers.get("x-ratelimit-reset")), tz=datetime.timezone.utc
            )
        except ValueError:
            # ISO 8601 (Mastodon, Jira, etc)
            api_reset = dateutil.parser.parse(r.headers.get("x-ratelimit-reset"))
        logging.debug(
            "Rate limit: {}/{} remaining until {} ({})".format(
                api_remaining,
                api_limit,
                api_reset,
                (api_reset - api_current_time),
            )
        )

        if api_remaining < 1:
            sleep_time = api_reset - api_current_time
        else:
            sleep_time = (api_reset - api_current_time) / (api_remaining + 1)
        return sleep_time

    def load_config(self):
        with self.args.config.open() as f:
            y = yaml.safe_load(f)
        self.data_dir = y.get("data_dir", ".")
        self.site = y.get("site")
        self.api_key = y.get("api_key")
        self.username = y.get("username")
        self.exclude_reblogs = y.get("exclude_reblogs", True)
        self.exclude_replies = y.get("exclude_replies", False)

    def parse_args(self, argv=None):
        if argv is None:
            argv = sys.argv

        parser = argparse.ArgumentParser(
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            prog=os.path.basename(argv[0]),
        )

        parser.add_argument(
            "--config",
            type=pathlib.Path,
            default="config.yaml",
            help="Configuration file",
        )

        parser.add_argument(
            "--refresh",
            type=int,
            default=0,
            help="Number of random downloaded posts to refresh",
        )

        parser.add_argument(
            "--debug",
            action="store_true",
            help="output additional debugging information",
        )

        return parser.parse_args(args=argv[1:])

    def download_attachments(self, item):
        created_at = dateutil.parser.parse(item["created_at"])
        for i, attachment in enumerate(item.get("media_attachments", []), start=1):
            ext = attachment["url"].split(".")[-1]
            fn = pathlib.Path(self.data_dir).joinpath(
                "{}-{}.{}".format(item["id"], i, ext)
            )
            r = self.requests_session.request("GET", attachment["url"])
            r.raise_for_status()
            logging.info(
                "{}: Saving {} attachment to {}".format(
                    item["id"], attachment["type"], fn
                )
            )
            with fn.open("wb") as f:
                for chunk in r.iter_content(chunk_size=1024):
                    if chunk:
                        f.write(chunk)
            os.utime(
                str(fn),
                (created_at.timestamp(), created_at.timestamp()),
            )

    def api_sleep(self, r):
        sleep_time = self.get_sleep_time(r)
        if sleep_time:
            logging.debug("Sleeping {} for rate limit".format(sleep_time))
            time.sleep(sleep_time.total_seconds())

    def refresh_posts(self, num_posts):
        rand_jsons = random.choices(
            list(pathlib.Path(self.data_dir).glob("*.json")), k=num_posts
        )
        for json_fn in rand_jsons:
            with json_fn.open() as f:
                saved_j = json.load(f)

            r = self.requests_session.request(
                "GET",
                "{}/api/v1/statuses/{}".format(self.site, saved_j["id"]),
                headers=self.api_headers,
            )
            try:
                r.raise_for_status()
            except requests.exceptions.HTTPError:
                if r.status_code == 404:
                    logging.info("{}: Post no longer exists".format(saved_j["id"]))
                    self.api_sleep(r)
                    continue
                else:
                    raise
            item = r.json()

            saved_edited_at = dateutil.parser.parse(
                saved_j["edited_at"] if saved_j["edited_at"] else saved_j["created_at"]
            )
            logging.debug(
                "{} saved: {}/{}/{}, {}".format(
                    saved_j["id"],
                    saved_j["replies_count"],
                    saved_j["reblogs_count"],
                    saved_j["favourites_count"],
                    saved_edited_at,
                )
            )
            edited_at = dateutil.parser.parse(
                item["edited_at"] if item["edited_at"] else item["created_at"]
            )
            logging.debug(
                "{} api: {}/{}/{}, {}".format(
                    item["id"],
                    item["replies_count"],
                    item["reblogs_count"],
                    item["favourites_count"],
                    edited_at,
                )
            )
            if (
                (edited_at > saved_edited_at)
                or (item["replies_count"] > saved_j["replies_count"])
                or (item["reblogs_count"] > saved_j["reblogs_count"])
                or (item["favourites_count"] > saved_j["favourites_count"])
            ):
                logging.info("{}: Saving updated post".format(item["id"]))
                self.save_item(item)
            self.api_sleep(r)

    def save_item(self, item):
        edited_at = dateutil.parser.parse(
            item["edited_at"] if item["edited_at"] else item["created_at"]
        )
        json_fn = pathlib.Path(self.data_dir).joinpath("{}.json".format(item["id"]))
        logging.info(
            "{}: Saving {} post from {} to {}".format(
                item["id"], item["visibility"], edited_at, json_fn
            )
        )
        self.download_attachments(item)
        with json_fn.open("w") as f:
            json.dump(item, f, sort_keys=True, indent=4)
        os.utime(str(json_fn), (edited_at.timestamp(), edited_at.timestamp()))

    def main(self):
        self.args = self.parse_args()
        logging.basicConfig(level=(logging.DEBUG if self.args.debug else logging.INFO))
        self.load_config()

        self.api_headers = {}
        if self.api_key:
            self.api_headers["Authorization"] = "Bearer {}".format(self.api_key)

        if self.username:
            r = self.requests_session.request(
                "GET",
                "{}/api/v1/accounts/lookup".format(self.site),
                params={"acct": self.username},
                headers=self.api_headers,
            )
            r.raise_for_status()
            acct = r.json()
            self.user_id = acct["id"]
            logging.info(
                "User is: {} ({})".format(acct["display_name"], acct["username"])
            )
        elif self.api_key:
            r = self.requests_session.request(
                "GET",
                "{}/api/v1/accounts/verify_credentials".format(self.site),
                headers=self.api_headers,
            )
            r.raise_for_status()
            me = r.json()
            self.user_id = me["id"]
            logging.info("I am {} ({})".format(me["display_name"], me["username"]))
        else:
            logging.error(
                "Neither username nor api_key specified, don't know who to look up"
            )
            return 1

        if self.args.refresh:
            return self.refresh_posts(self.args.refresh)

        try:
            last_json = sorted(pathlib.Path(self.data_dir).glob("*.json"))[-1]
            with last_json.open() as f:
                j = json.load(f)
            last_id = j["id"]
        except IndexError:
            last_id = "0"
        logging.info("Beginning at post {}".format(last_id))
        params = {
            "min_id": last_id,
            "exclude_reblogs": ("true" if self.exclude_reblogs else "false"),
            "exclude_replies": ("true" if self.exclude_replies else "false"),
        }
        url = "{}/api/v1/accounts/{}/statuses".format(self.site, self.user_id)
        while url:
            r = self.requests_session.request(
                "GET", url, params=params, headers=self.api_headers
            )
            r.raise_for_status()
            items = r.json()
            if not items:
                break
            for item in items:
                self.save_item(item)
            links = {
                x[1]: x[0] for x in self.re_links.findall(r.headers.get("link", ""))
            }
            url = links.get("prev")
            params = {}
            self.api_sleep(r)


if __name__ == "__main__":
    sys.exit(MastodonDownloader().main())
